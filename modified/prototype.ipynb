{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\OneDrive\\Desktop\\Attendace_System_Machine_Vision\\venv\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "Processing student images from: C:\\Users\\harik\\OneDrive\\Desktop\\Attendace_System_Machine_Vision\\Attendance_System\\Student_Images\n",
      "Found 2 student images to process\n",
      "Processing batch 1 with 2 images\n",
      "Reading image: hari.jpg\n",
      "Reading image: sakthi.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Successfully processed hari.jpg\n",
      "Successfully processed sakthi.jpg\n",
      "Saved 2 face encodings to C:\\Users\\harik\\OneDrive\\Desktop\\Attendace_System_Machine_Vision\\Attendance_System\\encodings.pkl\n",
      "Successfully created embeddings for 2 students\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from face_utils import (\n",
    "    STUDENT_IMAGES_FOLDER, \n",
    "    ENCODINGS_FILE, \n",
    "    get_face_analyzer,\n",
    "    build_attention_model,\n",
    "    enhance_image,\n",
    "    save_encodings,\n",
    "    batch_process_faces,\n",
    "    normalize,\n",
    "    is_good_quality_face\n",
    ")\n",
    "\n",
    "def create_student_embeddings():\n",
    "    \"\"\"Process all student images and create embeddings database\"\"\"\n",
    "    # Initialize models\n",
    "    face_analyzer = get_face_analyzer()\n",
    "    attention_model = build_attention_model()\n",
    "    attention_model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Arrays to store embeddings and names\n",
    "    student_encodings = []\n",
    "    student_names = []\n",
    "    \n",
    "    # Process each image in the student images folder\n",
    "    print(f\"Processing student images from: {STUDENT_IMAGES_FOLDER}\")\n",
    "    \n",
    "    # Collect images and names first\n",
    "    image_paths = []\n",
    "    names = []\n",
    "    \n",
    "    for filename in os.listdir(STUDENT_IMAGES_FOLDER):\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            image_path = os.path.join(STUDENT_IMAGES_FOLDER, filename)\n",
    "            student_name = os.path.splitext(filename)[0]  # Use filename (without extension) as student name\n",
    "            \n",
    "            image_paths.append(image_path)\n",
    "            names.append(student_name)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} student images to process\")\n",
    "    \n",
    "    # Process images in batches\n",
    "    batch_size = 10  # Process 10 images at a time\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_names = names[i:i+batch_size]\n",
    "        \n",
    "        print(f\"Processing batch {i//batch_size + 1} with {len(batch_paths)} images\")\n",
    "        \n",
    "        # Process each image in the batch\n",
    "        all_faces = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for j, (path, name) in enumerate(zip(batch_paths, batch_names)):\n",
    "            print(f\"Reading image: {os.path.basename(path)}\")\n",
    "            \n",
    "            # Read and enhance image\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"ERROR: Unable to read {path}\")\n",
    "                continue\n",
    "                \n",
    "            # Enhance image for low-light conditions\n",
    "            img = enhance_image(img)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Get faces from the image\n",
    "            faces = face_analyzer.get(img_rgb)\n",
    "            if len(faces) > 0:\n",
    "                face = faces[0]\n",
    "                \n",
    "                # Check if face is of good quality for registration\n",
    "                if not is_good_quality_face(face):\n",
    "                    print(f\"Low quality face in {os.path.basename(path)}: incorrect angle or pose, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Get bounding box for size validation\n",
    "                bbox = face.bbox.astype(int)\n",
    "                x, y, x2, y2 = bbox\n",
    "                \n",
    "                # Extract face crop to check dimensions\n",
    "                face_img = img_rgb[y:y2, x:x2]\n",
    "                \n",
    "                # Validate face crop dimensions\n",
    "                if face_img.size == 0 or face_img.shape[0] < 30 or face_img.shape[1] < 30:\n",
    "                    print(f\"Face crop too small in {os.path.basename(path)}: {face_img.shape}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                all_faces.append(face)\n",
    "                valid_indices.append(j)\n",
    "            else:\n",
    "                print(f\"No face detected in {path}\")\n",
    "        \n",
    "        if all_faces:\n",
    "            # Process all valid faces in batch\n",
    "            batch_embeddings = batch_process_faces(all_faces, attention_model)\n",
    "            \n",
    "            # Add embeddings and names to our lists\n",
    "            for idx, embedding in zip(valid_indices, batch_embeddings):\n",
    "                student_encodings.append(embedding)\n",
    "                student_names.append(batch_names[idx])\n",
    "                print(f\"Successfully processed {os.path.basename(batch_paths[idx])}\")\n",
    "    \n",
    "    # Save embeddings to pickle file\n",
    "    if student_encodings:\n",
    "        save_encodings(ENCODINGS_FILE, student_encodings, student_names)\n",
    "        print(f\"Successfully created embeddings for {len(student_encodings)} students\")\n",
    "    else:\n",
    "        print(\"No student embeddings were created. Check the images folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_student_embeddings() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded attendance sheet with 2 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harik\\OneDrive\\Desktop\\Attendace_System_Machine_Vision\\venv\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\harik/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "WARNING:tensorflow:5 out of the last 40 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024609AC74C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Detected asymmetric_occlusion with score 1.01\n",
      "Adjusting threshold for asymmetric_occlusion: 0.356 (was 0.552)\n",
      "The returned name: sakthi\n",
      "Detected asymmetric_occlusion with score 0.86\n",
      "Adjusting threshold for asymmetric_occlusion: 0.387 (was 0.552)\n",
      "The returned name: hari\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Detected asymmetric_occlusion with score 0.98\n",
      "Adjusting threshold for asymmetric_occlusion: 0.363 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.98)\n",
      "The returned name: sakthi\n",
      "The returned name: hari\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Detected asymmetric_occlusion with score 0.98\n",
      "Adjusting threshold for asymmetric_occlusion: 0.363 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.98)\n",
      "The returned name: sakthi\n",
      "Detected asymmetric_occlusion with score 0.78\n",
      "Adjusting threshold for asymmetric_occlusion: 0.401 (was 0.552)\n",
      "Maintaining identity hari through occlusion (score: 0.78)\n",
      "Matched occluded face with stored clear face: 0.94 > 0.45\n",
      "The returned name: hari\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Detected asymmetric_occlusion with score 0.96\n",
      "Adjusting threshold for asymmetric_occlusion: 0.367 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.96)\n",
      "The returned name: sakthi\n",
      "Detected asymmetric_occlusion with score 0.75\n",
      "Adjusting threshold for asymmetric_occlusion: 0.407 (was 0.552)\n",
      "Maintaining identity hari through occlusion (score: 0.75)\n",
      "Matched occluded face with stored clear face: 0.91 > 0.46\n",
      "The returned name: hari\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Detected asymmetric_occlusion with score 0.87\n",
      "Adjusting threshold for asymmetric_occlusion: 0.385 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.87)\n",
      "The returned name: sakthi\n",
      "Detected asymmetric_occlusion with score 0.79\n",
      "Adjusting threshold for asymmetric_occlusion: 0.400 (was 0.552)\n",
      "Maintaining identity hari through occlusion (score: 0.79)\n",
      "Matched occluded face with stored clear face: 0.92 > 0.45\n",
      "The returned name: hari\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.91\n",
      "Adjusting threshold for asymmetric_occlusion: 0.376 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.91)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.89\n",
      "Adjusting threshold for asymmetric_occlusion: 0.381 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.89)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.91\n",
      "Adjusting threshold for asymmetric_occlusion: 0.376 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.91)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.91\n",
      "Adjusting threshold for asymmetric_occlusion: 0.377 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.91)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.90\n",
      "Adjusting threshold for asymmetric_occlusion: 0.379 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.90)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.88\n",
      "Adjusting threshold for asymmetric_occlusion: 0.383 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.88)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "New anonymous face detected\n",
      "Saved 1 face encodings to C:\\Users\\harik\\OneDrive\\Desktop\\Attendace_System_Machine_Vision\\Attendance_System\\anonymous_faces.pkl\n",
      "The returned name: Anonymous_1\n",
      "Detected asymmetric_occlusion with score 0.90\n",
      "Adjusting threshold for asymmetric_occlusion: 0.379 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.90)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "The returned name: Anonymous_1\n",
      "Detected asymmetric_occlusion with score 0.91\n",
      "Adjusting threshold for asymmetric_occlusion: 0.376 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.91)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "The returned name: Anonymous_1\n",
      "Detected asymmetric_occlusion with score 0.89\n",
      "Adjusting threshold for asymmetric_occlusion: 0.380 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.89)\n",
      "The returned name: sakthi\n",
      "Face angle too steep: yaw=-49.1°, pitch=-15.6°, skipping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Detected asymmetric_occlusion with score 0.90\n",
      "Adjusting threshold for asymmetric_occlusion: 0.378 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.90)\n",
      "The returned name: sakthi\n",
      "Face angle too steep: yaw=-54.2°, pitch=-19.3°, skipping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Detected asymmetric_occlusion with score 0.91\n",
      "Adjusting threshold for asymmetric_occlusion: 0.377 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.91)\n",
      "The returned name: sakthi\n",
      "Face angle too steep: yaw=-83.5°, pitch=-10.8°, skipping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Detected asymmetric_occlusion with score 0.89\n",
      "Adjusting threshold for asymmetric_occlusion: 0.380 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.89)\n",
      "The returned name: sakthi\n",
      "Face angle too steep: yaw=-38.2°, pitch=4.8°, skipping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Detected asymmetric_occlusion with score 0.90\n",
      "Adjusting threshold for asymmetric_occlusion: 0.378 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.90)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.89\n",
      "Adjusting threshold for asymmetric_occlusion: 0.381 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.89)\n",
      "The returned name: sakthi\n",
      "Face angle too steep: yaw=-52.0°, pitch=-10.3°, skipping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Detected asymmetric_occlusion with score 0.90\n",
      "Adjusting threshold for asymmetric_occlusion: 0.379 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.90)\n",
      "The returned name: sakthi\n",
      "Face angle too steep: yaw=-41.3°, pitch=-2.3°, skipping\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Detected asymmetric_occlusion with score 0.83\n",
      "Adjusting threshold for asymmetric_occlusion: 0.391 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.83)\n",
      "The returned name: sakthi\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Detected asymmetric_occlusion with score 0.93\n",
      "Adjusting threshold for asymmetric_occlusion: 0.373 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.93)\n",
      "The returned name: sakthi\n",
      "Detected asymmetric_occlusion with score 0.80\n",
      "Adjusting threshold for asymmetric_occlusion: 0.397 (was 0.552)\n",
      "Maintaining identity hari through occlusion (score: 0.80)\n",
      "Matched occluded face with stored clear face: 0.83 > 0.45\n",
      "The returned name: hari\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Detected asymmetric_occlusion with score 0.77\n",
      "Adjusting threshold for asymmetric_occlusion: 0.403 (was 0.552)\n",
      "Maintaining identity hari through occlusion (score: 0.77)\n",
      "Matched occluded face with stored clear face: 0.80 > 0.46\n",
      "The returned name: hari\n",
      "Detected asymmetric_occlusion with score 0.92\n",
      "Adjusting threshold for asymmetric_occlusion: 0.375 (was 0.552)\n",
      "Maintaining identity sakthi through occlusion (score: 0.92)\n",
      "The returned name: sakthi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pdF\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from face_utils import (\n",
    "    STUDENT_IMAGES_FOLDER,\n",
    "    ENCODINGS_FILE,\n",
    "    ANONYMOUS_FILE,\n",
    "    ANONYMOUS_FOLDER,\n",
    "    ATTENDANCE_FILE,\n",
    "    THRESHOLD,\n",
    "    get_face_analyzer,\n",
    "    build_attention_model,\n",
    "    normalize,\n",
    "    enhance_image,\n",
    "    detect_backlighting,\n",
    "    is_blurry,\n",
    "    load_encodings,\n",
    "    save_encodings,\n",
    "    euclidean_distance,\n",
    "    add_padding_to_bbox,\n",
    "    batch_process_faces,\n",
    "    is_good_quality_face,\n",
    "    detect_mask_or_occlusion,\n",
    "    cosine_similarity,\n",
    "    adaptive_threshold,\n",
    "    FaceTracker,\n",
    "    find_best_match\n",
    ")\n",
    "\n",
    "# Array definitions\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "anonymous_encodings = []\n",
    "anonymous_names = []\n",
    "\n",
    "# Instantiate the attention model (SE block)\n",
    "attention_model = build_attention_model()\n",
    "attention_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "def create_encodings(folder_path):\n",
    "    # Initialize InsightFace model for creating the encodings\n",
    "    face_analyzer = get_face_analyzer()\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            path = os.path.join(folder_path, filename)\n",
    "            name = os.path.splitext(filename)[0]  # Use filename (without extension) as label\n",
    "            \n",
    "            # Read and process image\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"ERROR: Unable to read {path}\")\n",
    "                continue\n",
    "                \n",
    "            # Enhance image for low-light conditions\n",
    "            img = enhance_image(img)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Get faces from the image\n",
    "            faces = face_analyzer.get(img_rgb)\n",
    "            if len(faces) > 0:\n",
    "                # Use the first detected face's embedding\n",
    "                face = faces[0]\n",
    "                # Apply attention model to the embedding\n",
    "                embedding = attention_model.predict(face.embedding[np.newaxis, :])[0]\n",
    "                embedding = normalize(embedding)\n",
    "                \n",
    "                known_encodings.append(embedding)\n",
    "                known_names.append(name)\n",
    "            else:\n",
    "                print(f\"No face detected in {path}\")\n",
    "                continue\n",
    "\n",
    "# Add a new function to check if anonymous face is unique\n",
    "def is_unique_anonymous_face(embedding, existing_encodings, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check if a face embedding is sufficiently different from existing encodings\n",
    "    \n",
    "    Args:\n",
    "        embedding: New face embedding to check\n",
    "        existing_encodings: List of existing face embeddings to compare against\n",
    "        threshold: Similarity threshold (higher means more strict matching)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if this face is unique (not similar to any existing faces)\n",
    "    \"\"\"\n",
    "    if not existing_encodings:\n",
    "        return True\n",
    "        \n",
    "    # Calculate similarity with all existing anonymous encodings\n",
    "    for existing_embedding in existing_encodings:\n",
    "        similarity = cosine_similarity(embedding, existing_embedding)\n",
    "        # If similarity is above threshold, this face is similar to an existing one\n",
    "        if similarity > threshold:\n",
    "            return False\n",
    "            \n",
    "    # If no matches found, this face is unique\n",
    "    return True\n",
    "\n",
    "def attendance_system(start_time):\n",
    "    global df\n",
    "    global anonymous_encodings, anonymous_names\n",
    "    \n",
    "    # Keep track of which anonymous people are already in the Excel sheet\n",
    "    anonymous_in_excel = []\n",
    "    \n",
    "    # Track anonymous embeddings that are in the Excel sheet\n",
    "    anonymous_excel_encodings = []\n",
    "\n",
    "    # Load the InsightFace face analyzer\n",
    "    face_analyzer = get_face_analyzer()\n",
    "    \n",
    "    # Initialize face tracker for temporal consistency\n",
    "    face_tracker = FaceTracker(max_history=20, similarity_threshold=0.65)  # Maintain larger history for better tracking\n",
    "    \n",
    "    # Frame counter for tracking\n",
    "    frame_counter = 0\n",
    "    \n",
    "    # Map of track IDs to recognized names\n",
    "    track_names = {}\n",
    "    \n",
    "    # Confidence counters for each track\n",
    "    track_confidence = {}\n",
    "    \n",
    "    # Blur detection counter - tracks consecutive blurry frames\n",
    "    blur_counter = 0\n",
    "    \n",
    "    # Backlight detection counter\n",
    "    backlight_counter = 0\n",
    "    \n",
    "    # Track consecutive occlusion frames for the same person\n",
    "    occlusion_consistency_counter = {}\n",
    "    \n",
    "    # Store original embeddings for partially occluded faces\n",
    "    last_clear_face_embeddings = {}\n",
    "\n",
    "    # Start webcam\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    # Counter for anonymous faces\n",
    "    anonymous_counter = len(anonymous_encodings) + 1\n",
    "\n",
    "    # Check which anonymous names are already in Excel\n",
    "    if 'Name' in df.columns:\n",
    "        for name in df['Name']:\n",
    "            if isinstance(name, str) and name.startswith(\"Anonymous_\"):\n",
    "                anonymous_in_excel.append(name)\n",
    "                # Find the corresponding encoding\n",
    "                if name in anonymous_names:\n",
    "                    idx = anonymous_names.index(name)\n",
    "                    anonymous_excel_encodings.append(anonymous_encodings[idx])\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "            \n",
    "        # Increment frame counter\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Disabled blur detection\n",
    "        blur_counter = 0\n",
    "        frame_is_blurry = False\n",
    "        \n",
    "        # Check for backlighting and apply enhanced correction\n",
    "        frame_is_backlit = detect_backlighting(frame)\n",
    "        if frame_is_backlit:\n",
    "            backlight_counter += 1\n",
    "            if backlight_counter > 3:  # Apply enhanced correction after confirming backlighting\n",
    "                cv2.putText(frame, \"BACKLIT - ENHANCING\", (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "                # Apply enhanced image correction specifically for backlit images\n",
    "                frame = enhance_image(frame)\n",
    "        else:\n",
    "            backlight_counter = 0\n",
    "        \n",
    "        # Convert frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces using InsightFace\n",
    "        faces = face_analyzer.get(rgb_frame)\n",
    "        \n",
    "        if not faces:\n",
    "            # Display the frame with no faces\n",
    "            cv2.imshow(\"Face Recognition Attendance\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            continue\n",
    "            \n",
    "        # Process all face embeddings in batch\n",
    "        embeddings = batch_process_faces(faces, attention_model)\n",
    "        \n",
    "        # Process each face with corresponding embedding\n",
    "        for face, embedding in zip(faces, embeddings):\n",
    "            # Skip processing was removed - always process all faces\n",
    "                \n",
    "            # Extract face location\n",
    "            bbox = face.bbox.astype(int)\n",
    "            x, y, x2, y2 = bbox\n",
    "            w, h = x2 - x, y2 - y\n",
    "            \n",
    "            # Add padding to the bounding box\n",
    "            x1, y1, x2, y2 = add_padding_to_bbox(bbox, rgb_frame.shape)\n",
    "            face_img = rgb_frame[y1:y2, x1:x2]\n",
    "            \n",
    "            # Store the image in the face object for occlusion detection\n",
    "            face._img = face_img.copy()\n",
    "            \n",
    "            # Validate face crop dimensions\n",
    "            if face_img.size == 0 or face_img.shape[0] < 30 or face_img.shape[1] < 30:\n",
    "                print(f\"Face crop too small: {face_img.shape}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Removed blur check for face image\n",
    "                \n",
    "            # Check for face occlusion (mask)\n",
    "            is_occluded, occlusion_type, occlusion_score = detect_mask_or_occlusion(face)\n",
    "            if is_occluded:\n",
    "                print(f\"Detected {occlusion_type} with score {occlusion_score:.2f}\")\n",
    "                # Use more lenient threshold for masked faces\n",
    "                base_threshold = 0.65  # Lower threshold for masked faces\n",
    "            else:\n",
    "                base_threshold = 0.75  # Normal threshold for clear faces\n",
    "            \n",
    "            # For backlit images, adjust thresholds even further\n",
    "            if frame_is_backlit:\n",
    "                base_threshold *= 0.9  # Make threshold more lenient for backlit images\n",
    "                \n",
    "            # Get track ID for temporal consistency - pass the face object for occlusion detection\n",
    "            track_id = face_tracker.get_track_id(embedding, frame_counter, face=face)\n",
    "            \n",
    "            # Find the best match among known faces\n",
    "            name, score, is_match = find_best_match(\n",
    "                embedding, \n",
    "                known_encodings, \n",
    "                known_names, \n",
    "                face=face, \n",
    "                use_cosine=True,  # Use cosine similarity instead of Euclidean\n",
    "                base_threshold=base_threshold * 0.85  # Lower threshold for all face comparisons\n",
    "            )\n",
    "            \n",
    "            # Apply temporal consistency to the recognition\n",
    "            if track_id in track_names:\n",
    "                # We've seen this face before\n",
    "                prev_name = track_names[track_id]\n",
    "                \n",
    "                # Initialize occlusion counter if not present\n",
    "                if track_id not in occlusion_consistency_counter:\n",
    "                    occlusion_consistency_counter[track_id] = 0\n",
    "                \n",
    "                # If face is occluded, maintain previous identity more strongly\n",
    "                if is_occluded:\n",
    "                    occlusion_consistency_counter[track_id] += 1\n",
    "                    \n",
    "                    # Store occlusion score to use in decision making\n",
    "                    occlusion_weight = min(occlusion_score, 0.9)  # Cap at 0.9 to prevent complete override\n",
    "                    \n",
    "                    # If this face has been consistently occluded, favor previous identification\n",
    "                    # The higher the occlusion, the more we favor previous identification\n",
    "                    occlusion_threshold = 2\n",
    "                    if occlusion_score > 0.7:  # Heavy occlusion\n",
    "                        occlusion_threshold = 1  # Require fewer frames to maintain identity\n",
    "                        \n",
    "                    if occlusion_consistency_counter[track_id] >= occlusion_threshold:\n",
    "                        # Only stick with previous name if it's a named person (not Unregistered)\n",
    "                        if prev_name != \"Unregistered\" and not prev_name.startswith(\"Unknown\"):\n",
    "                            # For heavily occluded faces, always keep previous identity\n",
    "                            if occlusion_score > 0.5:\n",
    "                                name = prev_name\n",
    "                                is_match = True\n",
    "                                print(f\"Maintaining identity {prev_name} through occlusion (score: {occlusion_score:.2f})\")\n",
    "                    \n",
    "                    # If we have a clear face embedding stored for this track, compare with it\n",
    "                    if track_id in last_clear_face_embeddings:\n",
    "                        clear_embedding = last_clear_face_embeddings[track_id]\n",
    "                        similarity = cosine_similarity(embedding, clear_embedding)\n",
    "                        \n",
    "                        # If similar enough to the clear face, keep the identity\n",
    "                        # Use adaptive threshold based on occlusion level\n",
    "                        clear_face_threshold = 0.65 - (occlusion_weight * 0.25)\n",
    "                        if similarity > clear_face_threshold:\n",
    "                            name = prev_name\n",
    "                            is_match = True\n",
    "                            print(f\"Matched occluded face with stored clear face: {similarity:.2f} > {clear_face_threshold:.2f}\")\n",
    "                else:\n",
    "                    # Reset occlusion counter when face is clearly visible\n",
    "                    occlusion_consistency_counter[track_id] = 0\n",
    "                    \n",
    "                    # Store clear face embedding for future comparison during occlusion\n",
    "                    if not is_occluded or occlusion_score < 0.3:\n",
    "                        last_clear_face_embeddings[track_id] = embedding.copy()\n",
    "                \n",
    "                if prev_name == name:\n",
    "                    # Same identity detected again, increase confidence more aggressively\n",
    "                    if track_id not in track_confidence:\n",
    "                        track_confidence[track_id] = 2  # Start with higher confidence\n",
    "                    else:\n",
    "                        track_confidence[track_id] += 2  # Increase by 2 instead of 1\n",
    "                        \n",
    "                    # Cap confidence at higher value\n",
    "                    track_confidence[track_id] = min(track_confidence[track_id], 10)\n",
    "                else:\n",
    "                    # Different identity detected, decrease confidence more slowly\n",
    "                    if track_id not in track_confidence:\n",
    "                        track_confidence[track_id] = 0\n",
    "                    else:\n",
    "                        track_confidence[track_id] -= 0.5  # Decrease by 0.5 instead of 1\n",
    "                    \n",
    "                    # If confidence goes below zero, update the identity\n",
    "                    if track_confidence[track_id] < 0:\n",
    "                        track_names[track_id] = name\n",
    "                        track_confidence[track_id] = 1\n",
    "            else:\n",
    "                # First time seeing this face\n",
    "                track_names[track_id] = name\n",
    "                track_confidence[track_id] = 2  # Start with higher initial confidence\n",
    "                occlusion_consistency_counter[track_id] = 0\n",
    "            \n",
    "            # Update the track with this name for consistency tracking\n",
    "            face_tracker.update_track_name(track_id, name)\n",
    "            \n",
    "            # Get the most consistent name for this track\n",
    "            consistent_name = face_tracker.get_consistent_name(track_id, name)\n",
    "            \n",
    "            # Use the consistent name for display and attendance\n",
    "            name = consistent_name\n",
    "            \n",
    "            # Also update our track_names dictionary\n",
    "            track_names[track_id] = name\n",
    "            \n",
    "            # Flag to track if we should add this person to Excel\n",
    "            should_add_to_excel = True\n",
    "            \n",
    "            # If person is not identified as a student, check anonymous faces\n",
    "            if not is_match:\n",
    "                found_anonymous = False\n",
    "                \n",
    "                # Find the best match among anonymous faces using cosine similarity\n",
    "                anon_name, anon_score, anon_match = find_best_match(\n",
    "                    embedding, \n",
    "                    anonymous_encodings, \n",
    "                    anonymous_names, \n",
    "                    face=face, \n",
    "                    use_cosine=True,\n",
    "                    base_threshold=base_threshold * 0.93  # Slightly lower threshold for anonymous faces\n",
    "                )\n",
    "                \n",
    "                if anon_match:\n",
    "                    name = anon_name\n",
    "                    found_anonymous = True\n",
    "                    \n",
    "                    # Check if this anonymous person is already in Excel\n",
    "                    if name in anonymous_in_excel:\n",
    "                        should_add_to_excel = False\n",
    "                \n",
    "                # Register new anonymous face if not found and is good quality\n",
    "                if not found_anonymous:\n",
    "                    # Removed blur check for anonymous registration\n",
    "                    \n",
    "                    # Don't register anonymous faces from backlit frames without multiple confirmations\n",
    "                    if frame_is_backlit and backlight_counter < 5:\n",
    "                        name = \"Unregistered\"\n",
    "                        should_add_to_excel = False\n",
    "                        print(\"Not registering anonymous face from backlit frame without confirmation\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Don't require perfect face quality for anonymous registration if masked\n",
    "                    should_register = is_good_quality_face(face)\n",
    "                    \n",
    "                    # IMPORTANT: Never register anonymous faces with any occlusion\n",
    "                    # This prevents creating new anonymous identities when face is partially covered\n",
    "                    if is_occluded:\n",
    "                        # Don't register any anonymous faces when occlusion is detected\n",
    "                        should_register = False\n",
    "                        print(f\"Occlusion detected ({occlusion_score:.2f}), not registering new anonymous face\")\n",
    "                        \n",
    "                        # If track ID has a stored clear face, try to match with existing anonymous faces\n",
    "                        if track_id in last_clear_face_embeddings:\n",
    "                            clear_embedding = last_clear_face_embeddings[track_id]\n",
    "                            best_anon_match = None\n",
    "                            best_anon_similarity = 0.7  # Minimum similarity threshold\n",
    "                            \n",
    "                            # Search for best match among anonymous faces\n",
    "                            for i, anon_embedding in enumerate(anonymous_encodings):\n",
    "                                similarity = cosine_similarity(clear_embedding, anon_embedding)\n",
    "                                if similarity > best_anon_similarity:\n",
    "                                    best_anon_similarity = similarity\n",
    "                                    best_anon_match = anonymous_names[i]\n",
    "                            \n",
    "                            # If found a good match with stored clear face, use that identity\n",
    "                            if best_anon_match:\n",
    "                                name = best_anon_match\n",
    "                                print(f\"Matched occluded face with existing anonymous person: {name} ({best_anon_similarity:.2f})\")\n",
    "                    \n",
    "                    if should_register:\n",
    "                        # Use a higher similarity threshold (0.95) to reduce duplicate anonymous entries\n",
    "                        if is_unique_anonymous_face(embedding, anonymous_excel_encodings, threshold=0.95):\n",
    "                            print(f\"New anonymous face detected\")\n",
    "                            # Save enhanced version of the face\n",
    "                            enhanced_face_img = enhance_image(cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR))\n",
    "                            face_filename = os.path.join(ANONYMOUS_FOLDER, f\"Anonymous_{anonymous_counter}.jpg\")\n",
    "                            cv2.imwrite(face_filename, enhanced_face_img)\n",
    "                            \n",
    "                            anonymous_encodings.append(embedding)\n",
    "                            anon_name = f\"Anonymous_{anonymous_counter}\"\n",
    "                            anonymous_names.append(anon_name)\n",
    "                            save_encodings(ANONYMOUS_FILE, anonymous_encodings, anonymous_names)\n",
    "                            name = anon_name\n",
    "                            anonymous_counter += 1\n",
    "                            \n",
    "                            # Add to our tracking lists\n",
    "                            anonymous_excel_encodings.append(embedding)\n",
    "                            anonymous_in_excel.append(name)\n",
    "                            \n",
    "                            # Reset occlusion counter for this new identity\n",
    "                            occlusion_consistency_counter[track_id] = 0\n",
    "                        else:\n",
    "                            print(\"Similar anonymous face already exists in Excel, not adding a new entry\")\n",
    "                            should_add_to_excel = False\n",
    "                            # Find the most similar existing anonymous face\n",
    "                            best_similarity = -1\n",
    "                            best_match_name = None\n",
    "                            for i, existing_embedding in enumerate(anonymous_excel_encodings):\n",
    "                                similarity = cosine_similarity(embedding, existing_embedding)\n",
    "                                if similarity > best_similarity:\n",
    "                                    best_similarity = similarity\n",
    "                                    idx = anonymous_excel_encodings.index(existing_embedding)\n",
    "                                    best_match_name = anonymous_in_excel[i]\n",
    "                            \n",
    "                            if best_match_name:\n",
    "                                name = best_match_name\n",
    "                                print(f\"Matched with existing anonymous entry: {name}\")\n",
    "                            else:\n",
    "                                name = \"Unregistered\"\n",
    "                    else:\n",
    "                        # Not a good quality face for registration\n",
    "                        name = \"Unregistered\"\n",
    "                        should_add_to_excel = False\n",
    "                        print(\"Face not suitable for registration - poor angle or quality\")\n",
    "\n",
    "            print(\"The returned name:\", name)\n",
    "            \n",
    "            # Add occlusion indication if needed\n",
    "            display_name = name\n",
    "            if is_occluded:\n",
    "                display_name += \" (Masked)\"\n",
    "            \n",
    "            # Update attendance in Excel\n",
    "            current_time = datetime.now()\n",
    "            current_date = str(current_time.date())\n",
    "            current_time_str = str(current_time.strftime(\"%H:%M:%S\"))\n",
    "            \n",
    "            # Calculate time difference\n",
    "            time_difference = current_time - start_time\n",
    "            minutes_late = time_difference.total_seconds() / 60\n",
    "            \n",
    "            # Modified attendance update code to include anonymous persons\n",
    "            if name != \"Unregistered\" and should_add_to_excel:\n",
    "                # Check if the date column exists, if not create it\n",
    "                if current_date not in df.columns:\n",
    "                    df[current_date] = \"Absent\"  # Initialize with \"Absent\" for all\n",
    "                \n",
    "                # Check if this person is already in the dataframe for today\n",
    "                # Find the student's row\n",
    "                if name in df[\"Name\"].values:\n",
    "                    student_row = df[df[\"Name\"] == name].index[0]\n",
    "            \n",
    "                    # Update the attendance status if currently absent\n",
    "                    if df.at[student_row, current_date] == \"Absent\":\n",
    "                        if minutes_late > 20:\n",
    "                            df.at[student_row, current_date] = f\"Late ({current_time_str})\"\n",
    "                        else:\n",
    "                            df.at[student_row, current_date] = f\"Present ({current_time_str})\"\n",
    "                else:    \n",
    "                    # Add a new row for the student or anonymous person\n",
    "                    new_row = {\"Name\": name, current_date: f\"Present ({str(current_time_str)})\"}\n",
    "                    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    \n",
    "                    # If this is a new anonymous person, add to our tracking list\n",
    "                    if name.startswith(\"Anonymous_\") and name not in anonymous_in_excel:\n",
    "                        anonymous_in_excel.append(name)\n",
    "                        \n",
    "                df.to_excel(ATTENDANCE_FILE, index=False)\n",
    "            \n",
    "            # Draw rectangle and label on the frame with confidence\n",
    "            confidence_color = (0, 255, 0)  # Green for high confidence\n",
    "            \n",
    "            # For recognized students\n",
    "            if name in known_names:\n",
    "                confidence_text = f\" [Known]\"\n",
    "            elif name.startswith(\"Anonymous_\"):\n",
    "                confidence_text = f\" [Anon]\"\n",
    "                confidence_color = (255, 165, 0)  # Orange for anonymous\n",
    "            else:\n",
    "                confidence_text = f\" [??]\"\n",
    "                confidence_color = (0, 0, 255)  # Red for unrecognized\n",
    "                \n",
    "            # Add mask indicator\n",
    "            if is_occluded:\n",
    "                confidence_text += \" [Masked]\"\n",
    "                \n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), confidence_color, 2)\n",
    "            \n",
    "            # Display name with confidence info\n",
    "            label = f\"{display_name}{confidence_text}\"\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, confidence_color, 2)\n",
    "\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Face Recognition Attendance\", frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Load or create the attendance sheet\n",
    "    if os.path.exists(ATTENDANCE_FILE):\n",
    "        df = pd.read_excel(ATTENDANCE_FILE)\n",
    "        print(f\"Loaded attendance sheet with {len(df)} entries.\")\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Name\"])\n",
    "        print(\"Created new attendance sheet.\")\n",
    "    \n",
    "    # Load existing encodings\n",
    "    known_encodings, known_names = load_encodings(ENCODINGS_FILE)\n",
    "    anonymous_encodings, anonymous_names = load_encodings(ANONYMOUS_FILE)\n",
    "    \n",
    "    # Set start time for attendance\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run the attendance system\n",
    "    attendance_system(start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
